<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>YOLOv8 Object Detection</title>
    <!-- Bootstrap CSS -->
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css"
      rel="stylesheet"
    />
    <!-- Bootstrap icons CSS -->
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap-icons/1.10.5/font/bootstrap-icons.min.css"
    />

    <!-- OpenCV.js -->
    <script
      async
      src="https://docs.opencv.org/4.5.2/opencv.js"
      type="text/javascript"
    ></script>
    <!-- ONNX Runtime Web -->
    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/onnxruntime-web/1.20.1/ort.webgpu.min.js"
      integrity="sha512-VfhX+QkN7NbCrYehivVRUqfUdswaFcRPtmyBSMGIXGbIMRku82aqFB2mKxvrtNV9TYP6JWz371CfTIzAExMyCA=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>
    <style>
      body {
        background-color: #f8f9fa;
        color: #212529;
        /* overflow-y: hidden; */
      }

      .header {
        padding: 1.5rem 0;
        background: linear-gradient(to right, #4b6cb7, #182848);
        color: white;
        margin-bottom: 1.5rem;
      }

      .canvas-row {
        display: flex;
        justify-content: center;
        gap: 20px;
        margin-bottom: 1rem;
      }

      .detection-container {
        position: relative;
        width: 640px;
        height: 640px;
      }

      #canvas {
        position: absolute;
        top: 0;
        left: 0;
        z-index: 10;
      }

      #streamImages {
        position: absolute;
        top: 0;
        left: 0;
        z-index: 5;
      }

      .engine-container {
        position: relative;
        width: 640px;
        height: 640px;
      }

      .controls {
        background-color: white;
        border-radius: 8px;
        padding: 1rem;
        margin: 0 auto;
        max-width: 1300px;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
      }

      .form-switch .form-check-input {
        width: 3em;
      }

      @media (max-width: 1320px) {
        .canvas-row {
          flex-direction: column;
          align-items: center;
        }
      }
    </style>
  </head>
  <body>
    <!-- Header -->
    <div class="header text-center">
      <div class="container">
        <h1>Video Object Detection</h1>
        <p class="lead">
          YOLOv8 object detection application using <code>onnxruntime-web</code>
        </p>
      </div>
    </div>

    <div class="container-fluid">
      <!-- Centered Canvas Row -->
      <div class="canvas-row">
        <!-- Detection Canvas -->
        <div class="detection-container">
          <canvas id="canvas" width="640" height="640"></canvas>
          <img id="streamImages" width="640" height="640" />
        </div>

        <!-- Three.js Engine Canvas -->
        <div class="engine-container" id="data-engine-container">
          <!-- This is where Three.js will spawn its canvas -->
        </div>
      </div>

      <div class="row justify-content-center">
        <div class="col-lg-10">
          <!-- Checkbox Controls -->
          <div class="controls-section mb-3">
            <div class="d-flex justify-content-around">
              <div class="form-check form-switch">
                <input
                  class="form-check-input"
                  type="checkbox"
                  id="moveCamera"
                  checked
                />
                <label class="form-check-label" for="moveCamera">
                  <i class="bi bi-camera-video"></i> AI Camera Tracking
                </label>
              </div>
              <div class="form-check form-switch">
                <input
                  class="form-check-input"
                  type="checkbox"
                  id="showBoundingBoxes"
                  checked
                />
                <label class="form-check-label" for="showBoundingBoxes">
                  <i class="bi bi-bounding-box"></i> Show Bounding Boxes
                </label>
              </div>
              <div class="form-check form-switch">
                <input
                  class="form-check-input"
                  type="checkbox"
                  id="enableTracking"
                  checked
                />
                <label class="form-check-label" for="enableTracking">
                  <i class="bi bi-record-circle"></i> Enable Tracking
                </label>
              </div>
            </div>
          </div>

          <!-- Scene Object Movement Controls -->
          <div class="controls-section object-movement">
            <div class="d-flex justify-content-center">
              <button class="btn btn-light mx-2" id="moveLeft">
                <i class="bi bi-arrow-left-circle"></i>
              </button>
              <button class="btn btn-light mx-2" id="moveUp">
                <i class="bi bi-arrow-up-circle"></i>
              </button>
              <button class="btn btn-light mx-2" id="moveDown">
                <i class="bi bi-arrow-down-circle"></i>
              </button>
              <button class="btn btn-light mx-2" id="moveRight">
                <i class="bi bi-arrow-right-circle"></i>
              </button>
              <button class="btn btn-light mx-2" id="rotateLeft">
                <i class="bi bi-arrow-counterclockwise"></i>
              </button>
              <button class="btn btn-light mx-2" id="rotateRight">
                <i class="bi bi-arrow-clockwise"></i>
              </button>
              <button class="btn btn-light mx-2" id="zoomIn">
                <i class="bi bi-zoom-in"></i>
              </button>
              <button class="btn btn-light mx-2" id="zoomOut">
                <i class="bi bi-zoom-out"></i>
              </button>
            </div>
          </div>
        </div>
      </div>
    </div>

    <script type="importmap">
      {
        "imports": {
          "three": "https://ga.jspm.io/npm:three@0.174.0/build/three.module.js",
          "three/examples/jsm/loaders/GLTFLoader.js": "https://ga.jspm.io/npm:three@0.174.0/examples/jsm/loaders/GLTFLoader.js",
          "three/examples/jsm/loaders/FBXLoader.js": "https://ga.jspm.io/npm:three@0.174.0/examples/jsm/loaders/FBXLoader.js",
          "three/src/math/MathUtils.js": "https://ga.jspm.io/npm:three@0.174.0/src/math/MathUtils.js",
          "three/examples/jsm/libs/stats.module.js": "https://ga.jspm.io/npm:three@0.174.0/examples/jsm/libs/stats.module.js"
        }
      }
    </script>

    <script type="module">
      import * as THREE from "three";
      import { FBXLoader } from "three/examples/jsm/loaders/FBXLoader.js";
      import { GLTFLoader } from "three/examples/jsm/loaders/GLTFLoader.js";
      import { MathUtils } from "three/src/math/MathUtils.js";
      import * as Stats from "three/examples/jsm/libs/stats.module.js";
      // Global variables
      let stats = Stats.default.Panel();

      let session = null;
      let videoSource = null;
      let isPlaying = false;
      let modelLoaded = false;
      let camModel;
      const modelInputShape = [1, 3, 640, 640];
      const topk = 12;
      const iouThreshold = 0.45;
      const scoreThreshold = 0.25;
      const pointer = new THREE.Vector2();

      let objs = []; //includes fbx geo/mats
      //   const videoElement = document.getElementById("video");
      const canvasElement = document.getElementById("canvas");
      // canvasElement.style.translate = "0px 1000px";
      //   canvasElement.style.display = "none";
      const videoInput = document.getElementById("videoInput");

      const streamImages = document.getElementById("streamImages");
      const raycaster = new THREE.Raycaster();
      const AIFRAMES = 100;
      let rotationTarget = 0;

      // Initialize ONNX Runtime and OpenCV
      async function initialize() {
        // await cv["onRuntimeInitialized"];

        const arrBufNet = await download(`./yolov8n.onnx`); // put these two model files in the same directory as this html file
        const arrBufNMS = await download(`./nms-yolov8.onnx`);
        // const buff = await download('./line_follow.onnx');

        const yolov8 = await createSession(arrBufNet);
        const nms = await createSession(arrBufNMS);
        createFOSession(createFOSession);

        // Warmup model
        const tensor = new ort.Tensor(
          "float32",
          new Float32Array(modelInputShape.reduce((a, b) => a * b)),
          modelInputShape
        );
        await yolov8.run({ images: tensor });

        session = { net: yolov8, nms: nms };
        // console.log("Model loaded and ready.");

        modelLoaded = true;

        // console.log(stats.update);
        // document.body.appendChild(stats.dom);
      }

      // Create ONNX session
      async function createSession(arrBuf, backend = "webgpu") {
        const options = {
          executionProviders: [backend],
          intraOpNumThreads: navigator.hardwareConcurrency || 4,
          graphOptimizationLevel: "all",
          extra: {
            session: {
              disable_prepacking: "1",
              use_device_allocator_for_initializers: "1",
              use_ort_model_bytes_directly: "1",
              use_ort_model_bytes_for_initializers: "1",
            },
          },
        };

        try {
          ort.env.wasm.simd = true;
          const session = await ort.InferenceSession.create(arrBuf, options);
          return session;
        } catch (error) {
          console.warn(
            `Failed to create session with ${backend}. Falling back to default provider.`,
            error
          );

          return ort.InferenceSession.create(arrBuf, {
            intraOpNumThreads: navigator.hardwareConcurrency || 4,
          });
        }
      }

      // Configuration
      const IMAGE_SHAPE = { height: 66, width: 100, channels: 3 }; // Changed to 3 channels for RGB
      const MODEL_INPUT_SHAPE = {
        batch: -1, // Dynamic batch size
        height: 66,
        width: 100,
        channels: 3, // RGB
      };
      let ortSession = null;

      // Load ONNX model
      async function createFOSession(modelName) {
        try {
          // Replace with your model URL or local path
          ortSession = await ort.InferenceSession.create("line_follow.onnx");

          // Log model input details for debugging
          console.log("Model inputs:", ortSession.inputNames);
          // console.log('Input shape:', ortSession.inputs[0].dims);

          // document.getElementById('loading').textContent = 'Model loaded successfully!';
          // document.getElementById('predictBtn').disabled = false;
        } catch (e) {
          // document.getElementById('loading').textContent = `Failed to load model: ${e}`;
          console.error(e);
        }
      }

      export const download = (url, logger = null) => {
        return new Promise((resolve, reject) => {
          const request = new XMLHttpRequest();
          request.open("GET", url, true);
          request.responseType = "arraybuffer";
          request.onload = function () {
            if (this.status >= 200 && this.status < 300) {
              resolve(request.response);
            } else {
              reject({
                status: this.status,
                statusText: request.statusText,
              });
            }
            resolve(request.response);
          };
          request.onerror = function () {
            reject({
              status: this.status,
              statusText: request.statusText,
            });
          };
          request.send();
        });
      };

      async function preprocessImageFO(imageElement) {
        // const canvas = document.createElement('canvas');
        // canvas.width = MODEL_INPUT_SHAPE.width;
        // canvas.height = MODEL_INPUT_SHAPE.height;
        // const ctx = canvas.getContext('2d');

        // Draw and resize image (maintains aspect ratio with padding)
        const aspect = imageElement.naturalWidth / imageElement.naturalHeight;
        const targetAspect = MODEL_INPUT_SHAPE.width / MODEL_INPUT_SHAPE.height;

        if (aspect > targetAspect) {
          // Wider than target - scale to width and pad top/bottom
          const scaledHeight = MODEL_INPUT_SHAPE.width / aspect;
          // ctx.drawImage(
          //     imageElement,
          //     0, (MODEL_INPUT_SHAPE.height - scaledHeight) / 2,
          //     MODEL_INPUT_SHAPE.width, scaledHeight
          // );
        } else {
          // Taller than target - scale to height and pad left/right
          const scaledWidth = MODEL_INPUT_SHAPE.height * aspect;
          // ctx.drawImage(
          //     imageElement,
          //     (MODEL_INPUT_SHAPE.width - scaledWidth) / 2, 0,
          //     scaledWidth, MODEL_INPUT_SHAPE.height
          // );
        }

        // Get image data (RGB)
        const imageData = ctx.getImageData(
          0,
          0,
          MODEL_INPUT_SHAPE.width,
          MODEL_INPUT_SHAPE.height
        );

        // Create tensor data (normalized to [0, 1])
        const tensorData = new Float32Array(
          MODEL_INPUT_SHAPE.width *
            MODEL_INPUT_SHAPE.height *
            MODEL_INPUT_SHAPE.channels
        );

        for (let i = 0; i < imageData.data.length; i += 4) {
          const pixelIndex = Math.floor(i / 4);
          tensorData[pixelIndex * 3] = imageData.data[i] / 255.0; // R
          tensorData[pixelIndex * 3 + 1] = imageData.data[i + 1] / 255.0; // G
          tensorData[pixelIndex * 3 + 2] = imageData.data[i + 2] / 255.0; // B
        }

        // Create tensor with shape [1, height, width, channels]
        return new ort.Tensor("float32", tensorData, [
          1,
          MODEL_INPUT_SHAPE.height,
          MODEL_INPUT_SHAPE.width,
          MODEL_INPUT_SHAPE.channels,
        ]);
      }

      async function predictFO(tensor) {
        if (!ortSession) throw new Error("Model not loaded");
        // if (!fileList || fileList.length === 0) throw new Error('No files provided');

        try {
          // console.log(tensor)
          // Create batch tensor
          // updateProgress(70, 'Creating batch tensor...');
          // const batchSize = tensor.length;
          // const batchData = new Float32Array(
          //     batchSize *
          //     MODEL_INPUT_SHAPE.height *
          //     MODEL_INPUT_SHAPE.width *
          //     MODEL_INPUT_SHAPE.channels
          // );

          // let offset = 0;
          // tensors.forEach(tensor => {
          //     batchData.set(tensor.data, offset);
          //     offset += tensor.data.length;
          // });

          // const batchTensor = new ort.Tensor(
          //     'float32',
          //     batchData,
          //     [batchSize, MODEL_INPUT_SHAPE.height, MODEL_INPUT_SHAPE.width, MODEL_INPUT_SHAPE.channels]
          // );

          // Run inference
          // updateProgress(80, 'Running model inference...');
          const results = await ortSession.run({
            [ortSession.inputNames[0]]: tensor,
          });
          const predictions = results[ortSession.outputNames[0]].data;

          // Display results
          // updateProgress(90, 'Displaying results...');
          // for (let i = 0; i < validImages.length; i++) {
          //     document.getElementById(`pred-${i}`).textContent =
          //         `Prediction: ${predictions[i].toFixed(2)}`;
          // }

          // updateProgress(100, 'Prediction complete!');
          return predictions;
        } catch (error) {
          console.error("Prediction error:", error);
          // updateProgress(0, `Error: ${error.message}`);
          throw error;
        }
      }

      window.onload = async function () {
        await initialize();
      };
      let xRatio;
      /**
       * Detect Image
       * @param {HTMLImageElement} image Image to detect
       * @param {HTMLCanvasElement} canvas canvas to draw boxes
       * @param {ort.InferenceSession} session YOLOv8 onnxruntime session
       * @param {Number} topk Integer representing the maximum number of boxes to be selected per class
       * @param {Number} iouThreshold Float representing the threshold for deciding whether boxes overlap too much with respect to IOU
       * @param {Number} scoreThreshold Float representing the threshold for deciding when to remove boxes based on score
       * @param {Number[]} inputShape model input shape. Normally in YOLO model [batch, channels, width, height]
       */
      export const detectImage = async (
        image,
        session,
        topk,
        iouThreshold,
        scoreThreshold,
        inputShape
      ) => {
        const [modelWidth, modelHeight] = inputShape.slice(2);

        const tensor = new ort.Tensor("float32", image, inputShape); // to ort.Tensor
        const config = new ort.Tensor(
          "float32",
          new Float32Array([
            topk, // topk per class
            iouThreshold, // iou threshold
            scoreThreshold, // score threshold
          ])
        ); // nms config tensor
        const { output0 } = await session.net.run({ images: tensor }); // run session and get output layer
        const { selected } = await session.nms.run({
          detection: output0,
          config: config,
        }); // perform nms and filter boxes
        const boxes = [];
        // console.log(selected.dims[1]);
        // looping through output
        for (let idx = 0; idx < selected.dims[1]; idx++) {
          const data = selected.data.slice(
            idx * selected.dims[2],
            (idx + 1) * selected.dims[2]
          ); // get rows
          const box = data.slice(0, 4);
          const scores = data.slice(4); // classes probability scores
          const score = Math.max(...scores); // maximum probability scores
          const label = scores.indexOf(score); // class id of maximum probability scores

          const [x, y, w, h] = [
            (box[0] - 0.5 * box[2]) * xRatio, // upscale left
            (box[1] - 0.5 * box[3]) * 1, // upscale top
            box[2] * xRatio, // upscale width
            box[3] * 1, // upscale height
          ]; // keep boxes in maxSize range

          boxes.push({
            label: label,
            probability: score,
            bounding: [x, y, w, h], // upscale box
          }); // update boxes to draw later
        }
        await renderBoxes(boxes); // Draw boxes
        // input.delete(); // delete unused Mat
      };

      const labels = [
        "person",
        "bicycle",
        "car",
        "motorcycle",
        "airplane",
        "bus",
        "train",
        "truck",
        "boat",
        "traffic light",
        "fire hydrant",
        "stop sign",
        "parking meter",
        "bench",
        "bird",
        "cat",
        "dog",
        "horse",
        "sheep",
        "cow",
        "elephant",
        "bear",
        "zebra",
        "giraffe",
        "backpack",
        "umbrella",
        "handbag",
        "tie",
        "suitcase",
        "frisbee",
        "skis",
        "snowboard",
        "sports ball",
        "kite",
        "baseball bat",
        "baseball glove",
        "skateboard",
        "surfboard",
        "tennis racket",
        "bottle",
        "wine glass",
        "cup",
        "fork",
        "knife",
        "spoon",
        "bowl",
        "banana",
        "apple",
        "sandwich",
        "orange",
        "broccoli",
        "carrot",
        "hot dog",
        "pizza",
        "donut",
        "cake",
        "chair",
        "couch",
        "potted plant",
        "bed",
        "dining table",
        "toilet",
        "tv",
        "laptop",
        "mouse",
        "remote",
        "keyboard",
        "cell phone",
        "microwave",
        "oven",
        "toaster",
        "sink",
        "refrigerator",
        "book",
        "clock",
        "vase",
        "scissors",
        "teddy bear",
        "hair drier",
        "toothbrush",
      ];

      export const renderBoxes = async (boxes) => {
        // Verify canvas and context
        // if (!elem) {
        //   console.error("Canvas is null or undefined");
        //   return;
        // }

        // const ctx = elem.getContext("2d");
        // if (!ctx) {
        //   console.error("Failed to get 2d context");
        //   // Log canvas dimensions and state
        //   // console.log("Canvas dimensions:", {
        //   //   width: canvas.width,
        //   //   height: canvas.height,
        //   //   clientWidth: canvas.clientWidth,
        //   //   clientHeight: canvas.clientHeight,
        //   // });

        //   return;
        // }

        // Ensure canvas has dimensions
        // if (canvas.width === 0 || canvas.height === 0) {
        //   console.error("Canvas has zero dimensions");
        //   return;
        // }

        // Clear with logging
        // console.log("Clearing canvas...");
        ctx.clearRect(0, 0, ctx.canvas.width, ctx.canvas.height);

        const colors = new Colors();

        // Calculate and verify font size
        const fontSize = Math.max(Math.round(Math.max(640, 640) / 40), 14);
        // console.log("Using font size:", fontSize);

        // ctx.font = `${fontSize}px Arial`;
        // ctx.textBaseline = "top";

        // Add frame request to ensure rendering happens after GPU operations
        let mainBox = null;
        let maxArea = 0;

        boxes.forEach((box, index) => {
          // Log each box being processed
          // console.log(`Processing box ${index}:`, box);
          // if (labels[box.label] != "person"){ return}

          // console.log(labels[box.label]);
          const klass = labels[box.label];
          const color = colors.get(box.label);
          const score = (box.probability * 100).toFixed(1);
          const [x1, y1, width, height] = box.bounding;

          // Verify coordinates are valid numbers
          if ([x1, y1, width, height].some((n) => !Number.isFinite(n))) {
            console.error("Invalid coordinates for box:", box);
            return;
          }

          const area = width * height;
          if (area > maxArea) {
            maxArea = area;
            mainBox = box;
          }

          // Draw box with semi-transparent fill
          ctx.fillStyle = Colors.hexToRgba(color, 0.2);
          ctx.fillRect(x1, y1, width, height);

          // Draw border
          ctx.strokeStyle = color;
          ctx.lineWidth = Math.max(
            Math.min(ctx.canvas.width, ctx.canvas.height) / 200,
            2.5
          );
          ctx.strokeRect(x1, y1, width, height);

          // Calculate and verify text dimensions
          const labelText = `${klass} - ${score}%`;
          const textWidth = ctx.measureText(labelText).width;
          const textHeight = 10;
          const yText = y1 - (textHeight + ctx.lineWidth);

          // Draw label background
          ctx.fillStyle = color;
          ctx.fillRect(
            x1 - 1,
            yText < 0 ? 0 : yText,
            textWidth + ctx.lineWidth,
            textHeight + ctx.lineWidth
          );

          // Draw text
          ctx.fillStyle = "#000000";
          ctx.fillText(labelText, x1 - 1, yText < 0 ? 1 : yText + 1);
        });

        // Update camera rotation if camera is provided and we have a main box
        if (camera2 && mainBox) {
          const [x1, y1, width, height] = mainBox.bounding;

          // Convert box center to normalized device coordinates [-1, 1]
          const boxCenterX = ((x1 + width / 2) / 640) * 2 - 1;
          // const boxCenterY = -((y1 + height/2) / ctx.canvas.height * 2 - 1);

          // Calculate required rotation to keep box centered
          const targetRotationY = Math.atan2(boxCenterX, 1) * 1; // Reduced sensitivity

          // Calculate required rotation to keep full box in view
          // const boxRight = (x1 + width) / ctx.canvas.width * 2 - 1;
          // const boxLeft = x1 / ctx.canvas.width * 2 - 1;

          // Dynamic field-of-view adjustment
          // const boxWidthNDC = boxRight - boxLeft;
          // const fovAdjustment = Math.min(1.5, Math.max(0.7, 1/(boxWidthNDC * 0.7)));

          // Smooth transitions
          const lerpFactor = 0.8;
          // console.log("tr", targetRotationY )
          // console.log("r", camera.rotation.y )
          // camera.rotation.y = 1
          if (document.querySelector("#moveCamera").checked) {
            // camera.rotation.y = camera.rotation.y - targetRotationY;

            rotationTarget = camera2.rotation.y - targetRotationY;
          }
          // // Adjust FOV if camera supports it
          // if (camera.fov) {
          //   camera.fov = 50 * fovAdjustment;
          //   camera.updateProjectionMatrix();
          // }

          // // Optional: Adjust camera distance based on box size
          // const boxDiagonal = Math.sqrt(width * width + height * height);
          // const targetDistance = THREE.MathUtils.lerp(
          //   camera.position.z,
          //   5 + boxDiagonal * 0.05,
          //   lerpFactor
          // );
          // camera.position.z = targetDistance;
        }

        // await new Promise(requestAnimationFrame);

        // Final verification log
        // console.log("Rendering completed");
      };

      class Colors {
        // ultralytics color palette https://ultralytics.com/
        constructor() {
          this.palette = [
            "#FF3838",
            "#FF9D97",
            "#FF701F",
            "#FFB21D",
            "#CFD231",
            "#48F90A",
            "#92CC17",
            "#3DDB86",
            "#1A9334",
            "#00D4BB",
            "#2C99A8",
            "#00C2FF",
            "#344593",
            "#6473FF",
            "#0018EC",
            "#8438FF",
            "#520085",
            "#CB38FF",
            "#FF95C8",
            "#FF37C7",
          ];
          this.n = this.palette.length;
        }

        get = (i) => this.palette[Math.floor(i) % this.n];

        static hexToRgba = (hex, alpha) => {
          var result = /^#?([a-f\d]{2})([a-f\d]{2})([a-f\d]{2})$/i.exec(hex);
          return result
            ? `rgba(${[
                parseInt(result[1], 16),
                parseInt(result[2], 16),
                parseInt(result[3], 16),
              ].join(", ")}, ${alpha})`
            : null;
        };
      }

      const infoElement = document.getElementById("info");

      // Set up Three.js scene
      const scene = new THREE.Scene();
      const aspect = 1.77777777778;

      // Set reasonable dimensions for your orthographic view

      // Create OrthographicCamera
      const camera = new THREE.PerspectiveCamera(
        75,
        window.innerWidth / window.innerHeight,
        0.1,
        1000 // far
      );

      camera.layers.enable(1);

      const camera2 = new THREE.PerspectiveCamera(
        75,
        window.innerWidth / window.innerHeight,
        0.1,
        1000
      );

      const distance = 10; // Distance from center
      camera.position.set(distance, distance, distance);
      camera.lookAt(0, 0, 0);
      // camera2.layers.enable(2)
      // camera2.rotation.x = -0.5;
      // camera2.rotation.z =  -0.5;

      const renderer = new THREE.WebGLRenderer();
      renderer.setSize(640, 640);
      camera.aspect = window.innerWidth / window.innerHeight;
      renderer.domElement.style.position = "absolute";

      const container = document.getElementById('data-engine-container');

      // renderer.domElement.style.translate = "750px -730px";
      // Resize();
      // renderer.domElement.style.margin = "auto";
      // renderer.domElement.style.width = "50%";
      // renderer.domElement.style.height = "50%";

      renderer.setClearColor(0xffffff); //background color
      // renderer.setClearColor(0xf8f9fa); //background color
      // document.body.appendChild(renderer.domElement);
      container.appendChild(renderer.domElement);

      // Add some content to the scene
      const geometry = new THREE.BoxGeometry(2, 2, 2);

      const material = new THREE.MeshBasicMaterial({ color: 0xdddddd });
      const cube = new THREE.Mesh(geometry, material);
      // scene.add(cube);
      // cube.layers.set(1);

      const fbxLoader = new FBXLoader();
      const gltfLoader = new GLTFLoader();
      // Replace the box geometry code with FBX loading

      // fbxLoader.load(
      //   // Path to your FBX file
      //   "./running.fbx",

      //   // onLoad callback
      //   (fbx) => {
      //     // Scale and position the model if needed
      //     fbx.scale.set(1, 1, 1);
      //     fbx.position.set(0, 0, 0);

      //     // Add to scene
      //     scene.add(fbx);
      //     objs.push(fbx);

      //     // Optional: Traverse and adjust materials
      //     fbx.traverse((child) => {
      //       if (child.isMesh) {
      //         child.castShadow = true;
      //         child.receiveShadow = true;

      //         // Adjust material if needed
      //         if (child.material) {
      //           child.material.metalness = 0;
      //           child.material.roughness = 1;
      //         }
      //       }
      //     });

      //     // console.log("FBX model loaded successfully");
      //   },

      //   // onProgress callback
      //   (xhr) => {
      //     // console.log((xhr.loaded / xhr.total) * 100 + "% loaded");
      //   },

      //   // onError callback
      //   (error) => {
      //     console.error("Error loading FBX model:", error);
      //   }
      // );

      gltfLoader.load(
        // Path to your GLB file
        "./car.glb",

        // onLoad callback
        (gltf) => {
          // The loaded model is in gltf.scene
          const model = gltf.scene;

          // Scale and position the model
          model.scale.set(1, 1, 1); // Adjust as needed
          model.position.set(0, 0, 0);
          model.rotation.set(0, 0, 0);

          // Add to scene
          scene.add(model);
          objs.push(model);
          // Optional: Traverse and adjust materials/meshes
          model.traverse((child) => {
            if (child.isMesh) {
              // Enable shadows if needed
              child.castShadow = true;
              child.receiveShadow = true;

              // Adjust material properties
              if (child.material) {
                child.material.metalness = 0.1;
                child.material.roughness = 0.5;
                // child.material.envMap = environmentMap; // If using environment maps
              }
            }
          });

          // console.log("GLB model loaded successfully", model);

          // You can now access animations if your model has them:
          if (gltf.animations && gltf.animations.length) {
            const mixer = new THREE.AnimationMixer(model);
            const action = mixer.clipAction(gltf.animations[0]);
            action.play();

            // Add mixer to your update loop
            // function animate() {
            //   mixer.update(deltaTime);
            // }
          }
        },

        // onProgress callback
        (xhr) => {
          // console.log((xhr.loaded / xhr.total) * 100 + "% loaded");
        },

        // onError callback
        (error) => {
          console.error("Error loading GLB model:", error);
        }
      );

      gltfLoader.load(
        // Path to your GLB file
        "./camCar.glb",

        // onLoad callback
        (gltf) => {
          // The loaded model is in gltf.scene
          const model = gltf.scene;

          // Scale and position the model
          model.scale.set(1, 1, 1); // Adjust as needed
          model.position.set(
            camera2.position.x,
            camera2.position.y,
            camera2.position.z
          );
          model.rotation.set(0, 0, 0);
          model.layers.set(1);
          // Add to scene
          scene.add(model);

          camModel = model;
          // camModel.layers.set(1);
          // camModel.rotation.y = 3.14;
          // Optional: Traverse and adjust materials/meshes
          model.traverse((child) => {
            if (child.isMesh) {
              // Enable shadows if needed
              child.castShadow = true;
              child.receiveShadow = true;
              child.layers.set(1);

              // Adjust material properties
              if (child.material) {
                child.material.metalness = 0.1;
                child.material.roughness = 0.5;
                // child.material.envMap = environmentMap; // If using environment maps
              }
            }
          });

          // console.log("GLB model loaded successfully", model);

          // You can now access animations if your model has them:
          if (gltf.animations && gltf.animations.length) {
            const mixer = new THREE.AnimationMixer(model);
            const action = mixer.clipAction(gltf.animations[0]);
            action.play();

            // Add mixer to your update loop
            // function animate() {
            //   mixer.update(deltaTime);
            // }
          }
        },

        // onProgress callback
        (xhr) => {
          // console.log((xhr.loaded / xhr.total) * 100 + "% loaded");
        },

        // onError callback
        (error) => {
          console.error("Error loading GLB model:", error);
        }
      );

      camera2.position.z = 5;
      camera.position.z = 15;

      // Add a light source
      const light = new THREE.DirectionalLight(0xffffff, 1);
      light.position.set(1, 1, 1).normalize();
      scene.add(light);

      let lastResult = null;
      let frameCount = 0;

      // Animation loop
      async function animate() {
        requestAnimationFrame(animate);
        objs.forEach((element) => {
          // element.rotation.y -= 0.0005;
          element.position.x = 10 * Math.pow(Math.sin(frameCount / 600), 1);
        });

        camera2.rotation.y = rotationTarget;

        if (camModel) {
          camModel.rotation.y = camera2.rotation.y + 3.14;
        }

        renderer.render(scene, camera2);
        // Every N frames, send frame buffer to async
        if (frameCount % AIFRAMES === 0) {
          await captureFrameAndProcess();
        }
        frameCount++;

        renderer.render(scene, camera);

        // const cameraForward = new THREE.Vector3();

        // camera.getWorldDirection(cameraForward);

        // Set raycaster origin and direction

        // stats.update();
      }

      const ctx = canvasElement.getContext("2d", { willReadFrequently: true });
      // Function to capture frame buffer and send to worker
      async function captureFrameAndProcess() {
        if (!modelLoaded) return;
        let imgsrc = await renderer.domElement.toDataURL();
        // console.log(await predictFO(await(preprocessImageFO(imgsrc))));
        streamImages.src = imgsrc;
        streamImages.style.width = 640;
        streamImages.style.height = 640;
        // streamImages.style.translate = "300px 0px";
        streamImages.style.zIndex = 0;
        // let data = await prepareInputTensor(renderer);
        // let data = await captureAndPreprocess(renderer);
        // console.log(data);

        streamImages.onload = async function () {
          let mat = cv.imread(streamImages);
          // console.log(mat);
          const matC3 = new cv.Mat();
          cv.cvtColor(mat, matC3, cv.COLOR_RGBA2BGR);
          // //   console.log(mat)
          //   // 7. Add padding to make square
          const maxSize = Math.max(matC3.rows, matC3.cols);
          const matPad = new cv.Mat();

          xRatio = maxSize / matC3.cols; // set xRatio

          const xPad = maxSize - matC3.cols; // set xPadding
          const yPad = maxSize - matC3.rows; // set yPadding
          const yRatio = maxSize / matC3.rows; // set yRatio
          cv.copyMakeBorder(
            matC3,
            matPad,
            0,
            yPad,
            0,
            xPad,
            cv.BORDER_CONSTANT
          ); // padding black
          //   // 8. Create the final blob
          const blob = cv.blobFromImage(
            matC3,
            1.0 / 255.0, // Scale factor
            new cv.Size(640, 640), // Target size
            new cv.Scalar(0, 0, 0), // Mean subtraction
            true, // Swap RB channels
            false // Crop
          );
          //   console.log(matC3)

          //   // 9. Clean up OpenCV objects
          //   mat.delete();
          //   matC3.delete();
          //   matPad.delete();
          // console.log(matPad.data32F)

          //   matPad.data32F.forEach(element => {
          //         if(element !=0)
          //         {console.log(element)}
          //   });
          mat.delete();
          matC3.delete();
          matPad.delete();

          await detectImage(blob.data32F, session, 1, 0.01, 0.01, [
            1,
            3,
            canvasElement.width,
            canvasElement.height,
          ]);

          blob.delete();
        };
      }

      // Handle window resize
      // window.addEventListener("resize", function () {
      //   camera.aspect = window.innerWidth / window.innerHeight;
      //   camera.updateProjectionMatrix();
      //   console.log('r')
      //   renderer.setSize(640,640);
      // });

      let draggedObject;

      window.addEventListener("pointermove", onPointerDown);

      window.addEventListener("pointermove", onPointerMove);
      window.addEventListener("pointerup", onPointerUp);
   
      function onPointerDown(event) {
        // console.log("cl");
        // Calculate pointer position in normalized device coordinates
        // pointer.x = (event.clientX / window.innerWidth) * 2 - 1;
        // pointer.y = -(event.clientY / window.innerHeight) * 2 + 1;

        const rect = renderer.domElement.getBoundingClientRect();
const x = event.clientX - rect.left;
const y = event.clientY - rect.top;

pointer.x = ( x / canvas.clientWidth ) *  2 - 1;
pointer.y = ( y / canvas.clientHeight) * - 2 + 1

        checkIntersections(); // only functional in full screen
      }

      let prevX = 0;
      let prevY = 0;
      function onPointerMove(event) {
        if (draggedObject) {
          // Calculate mouse movement differences
          const mouseDeltaX = event.clientX - prevX;
          const mouseDeltaY = event.clientY - prevY;

          // Update previous positions
          prevX = event.clientX;
          prevY = event.clientY;

          // Camera-relative movement vectors
          const sensitivity = 0.05;

          // Get camera's right vector (for horizontal movement)
          const right = new THREE.Vector3();
          camera.getWorldDirection(right);
          right.cross(camera.up).normalize();

          // Get camera's forward vector (but projected to XZ plane for vertical movement)
          const forward = new THREE.Vector3();
          camera.getWorldDirection(forward);
          forward.y = 0;
          forward.normalize();

          // Calculate movement in world space
          const moveX = right.multiplyScalar(mouseDeltaX * sensitivity);
          const moveZ = forward.multiplyScalar(-mouseDeltaY * sensitivity); // Negative Y

          // Apply movement to object
          draggedObject.position.add(moveX);
          draggedObject.position.add(moveZ);

          // Lift object while dragging
          draggedObject.position.y = 0.7;
        } else {
          // Initialize previous positions
          prevX = event.clientX;
          prevY = event.clientY;
        }
      }

      function onPointerUp(event) {
        // console.log("stopped");
        if (draggedObject) {
          draggedObject.position.y = 0;
          draggedObject = null;
        }
      }

      // In your animation loop or click handler:
      function checkIntersections() {
        // Update the raycaster with pointer and camera
        raycaster.setFromCamera(pointer, camera);
        raycaster.layers.enable(1);
        // console.log(camera.layers,camera2.layers, camModel)
        console.log("wow")
        // Check intersections
        const intersects = raycaster.intersectObjects(scene.children);
        // Do something with intersections
        if (intersects.length > 0) {
          // intersects[0].object.material.wireframe = true;
          // intersects[0].object.position.y =
          // console.log(intersects)
          console.log("found object");
          intersects.forEach((element) => {
            // console.log(mostParent(element.object));
            draggedObject = mostParent(element.object);
            // draggedObject.position.y = 10;
            // console.log(a)
          });
          // console.log(intersects[0].object.parent.parent);

          // console.log(pointer.x,pointer.y)
        }
      }

      function mostParent(obj) {
        while (obj.parent) {
          if (obj.parent.type !== "Scene") {
            obj = obj.parent;
          } else {
            return obj;
          }
        }
        // console.log(obj.type == "Scene")
      }

      animate();
    </script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.2/js/bootstrap.bundle.min.js"></script>
  </body>
</html>
